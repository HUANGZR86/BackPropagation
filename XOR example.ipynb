{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "class XOR:\n",
    "    def __init__(self, inputs):\n",
    "        random.seed(1)\n",
    "        self.inputs=inputs\n",
    "        self.l=len(self.inputs) # equals 4, number of set of inputs\n",
    "        self.li=len(self.inputs[0]) #equals 2, number of input units\n",
    "        \n",
    "        self.wi=np.random.random((self.li,self.l)) #equals (2,4)\n",
    "        self.wh=np.random.random((self.l,1)) #equals (4, 1)\n",
    "    \n",
    "    def think(self, inp):\n",
    "        s1=sigmoid(np.dot(inp, self.wi))\n",
    "        s2=sigmoid(np.dot(s1, self.wh))\n",
    "        return s2\n",
    "    \n",
    "#back propagation:\n",
    "    def train(self, inputs, outputs, it):\n",
    "        for i in range(it):\n",
    "            l0=inputs\n",
    "            l1=sigmoid(np.dot(l0, self.wi))\n",
    "            l2=sigmoid(np.dot(l1, self.wh))\n",
    "            \n",
    "            l2_err=outputs - l2\n",
    "            l2_delta=np.multiply(l2_err, sigmoid_der(l2))\n",
    "            \n",
    "            l1_err=np.dot(l2_delta, self.wh.T)\n",
    "            l1_delta=np.multiply(l1_err, sigmoid_der(l1))\n",
    "            \n",
    "            self.wh+=np.dot(l1.T, l2_delta)\n",
    "            self.wi+=np.dot(l0.T, l1_delta)\n",
    "\n",
    "inputs=np.array([[0,0] , [0,1] , [1,0] , [1,1]])\n",
    "outputs=np.array( [ [0] , [1] , [1] , [0] ] )    \n",
    "n=XOR(inputs)\n",
    "print(\"Before training: \")\n",
    "print(n.think(inputs))\n",
    "n.train(inputs, outputs, 10000)\n",
    "print(\"After training: \")\n",
    "print(n.think(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X = (hours sleeping, hours studying), y = score on test\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "\n",
    "# scale units\n",
    "X = X/np.amax(X, axis=0) # maximum of X array\n",
    "y = y/100 # max test score is 100\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "    #parameters\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "\n",
    "    #weights\n",
    "        self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
    "\n",
    "    def forward(self, X):\n",
    "    #forward propagation through our network\n",
    "        self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o \n",
    "\n",
    "    def sigmoid(self, s):\n",
    "    # activation function \n",
    "        return 1/(1+np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "    #derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "    # backward propgate through the network\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "        self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "        self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "\n",
    "    def train (self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "NN = Neural_Network()\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "    print(\"Input: \\n\" + str(X)) \n",
    "    print(\"Actual Output: \\n\" + str(y))\n",
    "    print( \"Predicted Output: \\n\" + str(NN.forward(X)) )\n",
    "    print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n",
    "    print (\"\\n\")\n",
    "    NN.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "# define sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "# input dataset 总共有4组输入\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "\n",
    "# output dataset 四组输出\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "#seed random numbers to make calculation\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "#theta0 是一个3*4的参数矩阵,输入层有3个输入节点(神经元)，隐藏层有4个节点(神经元)，所以参数是3*4d的矩阵\n",
    "theta0 = 2*np.random.random((3,4)) - 1\n",
    "\n",
    "#theta1是一个4*1的参数矩阵，隐藏层有4个节点(神经元)，输出层有1个节点(神经元)\n",
    "theta1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for j in range(60000):\n",
    "    a0 = X  #a0表示第一层(输入层)，a0的每一行表示一组输入数据\n",
    "    a1 = nonlin(np.dot(a0,theta0)) #a0与theta0的相乘得到的就是z1，经过sigmod函数就是a1了。a1是4*4的矩阵。注意是批量运算，a1的每一行代表一组训练数据。\n",
    "    a2 = nonlin(np.dot(a1,theta1)) #跟上一个语句一样的道理,a2是一个4*1的矩阵，每一行代表一组训练数据\n",
    "\n",
    "    E = y - a2 #获得偏差E，4*1的矩阵，每一行代表一组训练数据\n",
    "\n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Error: \" + str(np.mean(np.abs(E))))\n",
    "\n",
    "    a2_delta = E*nonlin(a2,deriv=True) #这才是点乘，对应元素相乘，a2_delta是一个4*1的矩阵，每一行代表一组训练数据\n",
    "    a1_error = a2_delta.dot(theta1.T) # a1__error其实就是偏差对a1求偏导对应输出层的E，也是每一行代表一组训练数据\n",
    "    a1_delta = a1_error*nonlin(a1,deriv=True)\n",
    "\n",
    "    theta1 = theta1 + a1.T.dot(a2_delta)\n",
    "    theta0 = theta0 + a0.T.dot(a1_delta)\n",
    "\n",
    "print(\"output after training:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "vec_sigmoid = np.vectorize(sigmoid)\n",
    "\n",
    "input_sz = 2;\n",
    "hidden_sz = 3;\n",
    "output_sz = 1;\n",
    "theta1 = np.matrix(0.5 * np.sqrt(6.0 / (input_sz+hidden_sz)) * (np.random.rand(1+input_sz,hidden_sz)-0.5))\n",
    "theta2 = np.matrix(0.5 * np.sqrt(6.0 / (hidden_sz+output_sz)) * (np.random.rand(1+hidden_sz,output_sz)-0.5))\n",
    "\n",
    "def fit(x, y, theta1, theta2, learn_rate=.1):\n",
    "    #forward pass\n",
    "    layer1 = np.matrix(x, dtype='f')\n",
    "    layer1 = np.c_[np.ones(1), layer1]\n",
    "    # Binesh - for layer2 we need to add a bias term.\n",
    "    layer2 = np.c_[np.ones(1), vec_sigmoid(layer1.dot(theta1))]\n",
    "    layer3 = sigmoid(layer2.dot(theta2))\n",
    "\n",
    "    #backprop\n",
    "    delta3 = y - layer3\n",
    "    # Binesh - In reality, this is the _negative_ derivative of the cross entropy function\n",
    "    # wrt the _input_ to the final sigmoid function.\n",
    "\n",
    "    delta2 = np.multiply(delta3.dot(theta2.T), np.multiply(layer2, (1-layer2)))\n",
    "    # Binesh - We actually don't use the delta for the bias term. (What would be the point?\n",
    "    # it has no inputs. Hence the line below.\n",
    "    delta2 = delta2[:,1:]\n",
    "\n",
    "    # But, delta's are just derivatives wrt the inputs to the sigmoid.\n",
    "    # We don't add those to theta directly. We have to multiply these by\n",
    "    # the preceding layer to get the theta2d's and theta1d's\n",
    "    theta2d = np.dot(layer2.T, delta3)\n",
    "    theta1d = np.dot(layer1.T, delta2)\n",
    "\n",
    "    #update weights\n",
    "    # Binesh - here you had delta3 and delta2... Those are not the\n",
    "    # the derivatives wrt the theta's, they are the derivatives wrt\n",
    "    # the inputs to the sigmoids.. (As I mention above)\n",
    "    theta2 += learn_rate * theta2d #??\n",
    "    theta1 += learn_rate * theta1d #??\n",
    "\n",
    "def train(X, Y):\n",
    "    for _ in range(10000):\n",
    "        for i in range(4):\n",
    "            x = X[i]\n",
    "            y = Y[i]\n",
    "            fit(x, y, theta1, theta2)\n",
    "\n",
    "\n",
    "# Binesh - Here's a little test function to see that it actually works\n",
    "def test(X):\n",
    "    for i in range(4):\n",
    "        layer1 = np.matrix(X[i],dtype='f')\n",
    "        layer1 = np.c_[np.ones(1), layer1]\n",
    "        layer2 = np.c_[np.ones(1), vec_sigmoid(layer1.dot(theta1))]\n",
    "        layer3 = sigmoid(layer2.dot(theta2))\n",
    "       \n",
    "\n",
    "X = [(0,0), (1,0), (0,1), (1,1)]\n",
    "Y = [0, 1, 1, 0]    \n",
    "train(X, Y)\n",
    "print(train(X,Y))\n",
    "\n",
    "# Binesh - Alright, let's see!\n",
    "print(test(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
